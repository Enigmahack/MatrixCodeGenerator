# GLSL Rendering Pipeline Bugs & Refactors

Based on a review of the `WebGLRenderer.js` and the quantized GLSL rendering pipeline, here are several bugs and mathematical inconsistencies that affect rendering accuracy, customization, and visual stability.

## 1. Color Space / Alpha Blending Inconsistency (Composite Mode)
*   **Bug/Issue:** In the `lineFS` shader (Mode 1), the compositing logic adds the line color directly to the base character color: `resultColor += lineBaseColor * lineIntensity;`. This is an additive, premultiplied-style operation. However, the final output sets the alpha channel to `max(base.a, lineAlpha)`. If the downstream rendering pipeline (or standard WebGL blending) applies `gl.SRC_ALPHA`, the already-scaled RGB values will be multiplied by alpha *again*, resulting in a double-multiplication that causes lines to look unexpectedly dim, dark halos, and distorted color curves.
*   **Recommended Solution:** Standardize the color space. If the pipeline expects non-premultiplied alpha, use a standard mix function: `resultColor = mix(resultColor, lineBaseColor, lineAlpha);`. If additive blending is desired, ensure the downstream pipeline expects premultiplied RGB and bypass `max()` logic that forces a dense alpha channel.

## 2. SDF Resolution Dependency (Non-Uniform Scaling)
*   **Bug/Issue:** In the `lineFS` shader (Mode 0), the Signed Distance Field (SDF) used to draw the quantized lines calculates its distance `d` in *logic grid cell coordinates* (`p = (logicPos - nearestI) * u_cellPitch;`). Because `u_thickness` and `u_glowFalloff` are applied against this cell-space distance, the visual thickness and glow of the lines will dynamically warp and stretch if the grid cells are not perfectly square (e.g., upon window resize or aspect ratio shifts). A thickness of "1.0" does not map to a consistent pixel width.
*   **Recommended Solution:** Convert the local block coordinates to absolute screen pixel coordinates before calculating the SDF. Multiply `p` and the line segment endpoints by `(u_screenStep / u_cellPitch)` so that `d` is evaluated in absolute pixels. Update `u_thickness` and `u_glowFalloff` to map directly to pixel scales.

## 3. Persistence Buffer Stall (8-bit Quantization Error)
*   **Bug/Issue:** The hardware line fade effect subtracts a fractional value (`1.0 / persistence_frames`) from the persistence FBO every frame using `gl.FUNC_REVERSE_SUBTRACT`. If the user's hardware does not support float textures (`EXT_color_buffer_float`), the buffer falls back to 8-bit unsigned bytes. If the `Persistence` setting is high enough (e.g., > 255 frames), the subtraction value becomes `< 1.0 / 255.0`. In 8-bit math, this quantizes to `0`, causing the fading lines to freeze and remain permanently burned onto the screen.
*   **Recommended Solution:** Enforce a minimum subtraction delta of `1.0 / 255.0` (or `ceil(persistence * 255.0) / 255.0`) when using 8-bit fallback textures. Alternatively, abandon the iterative subtract method and pass the exact birth-frame and current-frame to the shader to calculate exact opacity mathematically (`max(0, 1.0 - (now - birthFrame)/duration)`).

## 4. Attribute Overloading Conflict (`v_glow` vs `nwA`)
*   **Bug/Issue:** In the `matrixFS` shader, during a Dual World / Shadow Transition (`useMix >= 5.0`), the varying `v_glow` is hijacked to represent the New World Alpha (`float nwA = v_glow;`). However, later in the same fragment shader, `v_glow` is still processed as an emissive glow multiplier (`col.rgb += glowFactor * 0.3 * col.a;`). This means any opaque New World character will trigger massive, unintended bloom and brightness simply because its alpha value is being interpreted as an emissive glow value.
*   **Recommended Solution:** Stop overloading `v_glow` for dual purposes. There are plenty of unused or mutually exclusive vertex attributes (e.g., the fractional part of `v_mix`, or an entirely new attribute) that can be used to pass the secondary alpha. Alternatively, add a conditional check to disable standard glow additions when `useMix >= 5.0`.

## 5. Redundant Fade Logic / Double Decay
*   **Bug/Issue:** The WebGL pipeline for quantized lines uses a dedicated hardware Persistence Buffer to automatically fade lines over time. However, the CPU logic in `_renderQuantizedLineGfx` simultaneously fades the `alpha` value it writes to the `u_logicGrid` texture for removed blocks based on the `maxFadeOut` duration. Feeding a manually fading signal into an automatically decaying hardware buffer creates a "double-decay," resulting in non-linear, unpredictable fade curves.
*   **Recommended Solution:** Decouple the CPU fade logic from the GPU persistence buffer. When rendering in WebGL mode, removed blocks should be instantly cleared from `u_logicGrid` (`alpha = 0`), allowing the hardware persistence buffer to handle 100% of the fading natively. Only use the CPU-driven alpha interpolation for the 2D Canvas fallback.